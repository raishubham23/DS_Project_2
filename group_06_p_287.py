# -*- coding: utf-8 -*-
"""Group-06, P_287.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xPW5dtJpovslhXHJzM_9YtAF8ritiZXv
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.holtwinters import ExponentialSmoothing

from pandas.plotting import lag_plot
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')

from google.colab import files
uploaded = files.upload()

df= pd.read_csv("Website Vistiors Daywise - Sheet1.csv")
df.head()

# changing the type of "Date" column from object to datetime
df["Date"]= pd.to_datetime(df["Date"])

df.dtypes

df.shape

df.info

df.isnull().sum()

df.duplicated().sum()

df.describe()

# Rename the column
# df.rename(columns={'date': 'Date'}, inplace=True)

df.rename(columns={'Daily Visitors': 'Visitors'}, inplace=True)

df

"""## Outlier Analysis and Removal"""

import pandas as pd

# Assuming you already have a DataFrame named df

Q1 = df['Visitors'].quantile(0.25)
Q3 = df['Visitors'].quantile(0.75)

IQR = Q3 - Q1

lower_limit = Q1 - 1.5 * IQR
upper_limit = Q3 + 1.5 * IQR

outliers = df[(df['Visitors'] < lower_limit) | (df['Visitors'] > upper_limit)]
df = df[~((df['Visitors'] < lower_limit) | (df['Visitors'] > upper_limit))]

# Now df contains the data with outliers removed, and outliers are stored in the 'outliers' DataFrame.

df.shape

df.dtypes

plt.figure(figsize=(8, 4))
plt.subplot(211)
sns.boxplot(df.Visitors)
plt.subplot(212)
sns.histplot(df.Visitors, kde=True)  # You can use histplot with kde=True for a similar visualization
plt.show()

"""## Checking for Stationarity"""

# Create a plot with 'Date' as the x-axis and 'Visitors' as the y-axis
df.plot(x= 'Date', y='Visitors', legend=False)  # legend=False removes the legend
plt.xlabel('Date')
plt.ylabel('Visitors')
plt.title('Visitors Over Time')

# Dickey_fuller test
from statsmodels.tsa.stattools import adfuller

# df.set_index('Date', inplace=True)

df.head()

# Define a function to perform the ADF test and interpret the results
def adf_test(df):
    res = adfuller(df, autolag='AIC')
    print('Augmented Dickey-Fuller Statistic: %f' % res[0])
    print('p-value: %f' % res[1])
    print('Lags: %f' % res[2])

    # Printing the critical values at different alpha levels.
    print('Critical values at different levels:')
    for k, v in res[4].items():
        print('\t%s: %.3f' % (k, v))

    result = 'stationary' if res[1] <= 0.05 else 'non-stationary'
    print(f'Result: The data is {result}')

# Perform the ADF test on the 'visitors' column
adf_test(df['Visitors'])

df1= df

'''# Assuming you already have a DataFrame named df with "Date" and "Visitors" columns

# Apply differencing only to the "Visitors" column
df1['Visitors'] = df1['Visitors'].diff()

# Now, df['Visitors_diff'] contains the differences between consecutive values in the "Visitors" column'''

import pandas as pd

# Assuming you have a DataFrame named df1 with a 'Visitors' column

# Create a new column to store the differences
df1['Visitors_diff'] = df1['Visitors'].diff()

df1.isnull().sum()

df1.dropna(inplace= True)

df1.isnull().sum()

df1

# Create a plot with 'Date' as the x-axis and 'Visitors' as the y-axis
df1.plot(x= 'Date', y='Visitors', legend=False)  # legend=False removes the legend
plt.xlabel('Date')
plt.ylabel('Visitors')
plt.title('Visitors Over Time')

'''adf1= adfuller(df1.dropna())
print('p_value of ADF test is :', adf1[1]) # 0.0002 < 0.05'''

# Define a function to perform the ADF test and interpret the results
def adf_test(df1):
    res = adfuller(df1, autolag='AIC')
    print('Augmented Dickey-Fuller Statistic: %f' % res[0])
    print('p-value: %f' % res[1])
    print('Lags: %f' % res[2])

    # Printing the critical values at different alpha levels.
    print('Critical values at different levels:')
    for k, v in res[4].items():
        print('\t%s: %.3f' % (k, v))

    result = 'stationary' if res[1] <= 0.05 else 'non-stationary'
    print(f'Result: The data is {result}')

# Perform the ADF test on the 'visitors' column
adf_test(df1['Visitors_diff'])

# Kwiatkowski-Phillips-Schmidt-Shin(KPSS)
from statsmodels.tsa.stattools import kpss

def kpss_test(df1, regression="c", nlags="auto"):
    statistic, p_value, n_lags, critical_values = kpss(df1, regression=regression, nlags=nlags)

    # Format Output
    print(f'KPSS Statistic: {statistic}')
    print(f'p-value: {p_value}')
    print(f'num lags: {n_lags}')
    print('Critical Values:')
    for key, value in critical_values.items():
        print(f'   {key} : {value}')

    result = 'stationary' if p_value >= 0.05 else 'not stationary'
    print(f'Result: The data is {result}')

# Perform the KPSS test on the 'visitors' column with customized arguments
kpss_test(df1['Visitors_diff'], regression="c", nlags="auto")

"""#### Based on the findings from both tests, it can be concluded that the time series exhibits stationarity, indicating the absence of both a deterministic trend and seasonality within the series."""

ts_mul = seasonal_decompose(df1.Visitors,model="Additive", period =50)
fig = ts_mul.plot()
plt.show()

"""## ACF and PACF Plot"""

# Create an autocorrelation plot
from statsmodels.graphics.tsaplots import plot_acf
import statsmodels.graphics.tsaplots as tsa_plots
tsa_plots.plot_acf(df1.Visitors, lags= 50)
tsa_plots.plot_pacf(df1.Visitors,lags= 50)
plt.show();

"""## Model Splitting"""

# Splitting the time series obtained after transformation
X = df1['Visitors_diff']
train, test = np.split(X, [int(0.70 * len(X))])

print("train dimensions", train.shape)
print("test dimensions", test.shape)

train_data, test_data = train, test

"""## Model Building

## 1. ARIMA
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_absolute_error, mean_squared_error

# df = pd.read_csv('your_data.csv', parse_dates=['Date'], index_col='Date')

# ARIMA Model
arima_model = ARIMA(train_data, order=(2, 1, 1))  # Example order (p, d, q)
arima_fit = arima_model.fit()

# Calculate the length of the test_data
test_data_length = len(test_data)

# Set the end argument for predict based on the last date in test_data; Make ARIMA predictions
pred_arima = arima_fit.predict(start=test_data.index[0], end=test_data.index[-1], typ='levels')[:test_data_length]

# Calculate evaluation metrics for ARIMA
mae_arima = mean_absolute_error(test_data, pred_arima)
mse_arima = mean_squared_error(test_data, pred_arima)
rmse_arima = np.sqrt(mse_arima)

# Plot ARIMA results
plt.figure(figsize=(12, 6))
plt.plot(train_data, label='Training Data')
plt.plot(test_data, label='Test Data')
plt.plot(pred_arima, label='ARIMA Forecast', color='red')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Value")
plt.title("ARIMA Forecast")
plt.show()

print("ARIMA Mean Absolute Error:", mae_arima)
print("ARIMA Mean Squared Error:", mse_arima)
print("ARIMA Root Mean Squared Error:", rmse_arima)

pip install pmdarima

"""## 2. Exponential Smoothing"""

from statsmodels.tsa.holtwinters import ExponentialSmoothing
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Load your data into a pandas DataFrame
# Ensure that you have a datetime index for time series data
# For example:
# df = pd.read_csv('your_data.csv', parse_dates=['Date'], index_col='Date')

# Exponential Smoothing Model
es_model = ExponentialSmoothing(train_data, seasonal='add', seasonal_periods=7)  # Example settings
es_fit = es_model.fit()

# Make Exponential Smoothing predictions
pred_es = es_fit.forecast(steps=len(test_data))

# Calculate evaluation metrics for Exponential Smoothing
mae_es = mean_absolute_error(test_data, pred_es)
mse_es = mean_squared_error(test_data, pred_es)
rmse_es = np.sqrt(mse_es)

# Plot Exponential Smoothing results
plt.figure(figsize=(12, 6))
plt.plot(train_data, label='Training Data')
plt.plot(test_data, label='Test Data')
plt.plot(pred_es, label='Exponential Smoothing Forecast', color='red')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Value")
plt.title("Exponential Smoothing Forecast")
plt.show()

print("Exponential Smoothing Mean Absolute Error:", mae_es)
print("Exponential Smoothing Mean Squared Error:", mse_es)
print("Exponential Smoothing Root Mean Squared Error:", rmse_es)

"""## 3. SARIMA"""

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX

# SARIMA Model
sarima_model = SARIMAX(train_data, order=(1, 1, 1), seasonal_order=(0, 1, 1, 7))  # Example orders (p, d, q) and seasonal_order (P, D, Q, S)
sarima_fit = sarima_model.fit()

# Calculate the length of the test_data
test_data_length = len(test_data)

# Make SARIMA predictions
pred_sarima = sarima_fit.predict(start=test_data.index[0], end=test_data.index[-1], typ='levels')[:test_data_length]

# Calculate evaluation metrics for SARIMA
mae_sarima = mean_absolute_error(test_data, pred_sarima)
mse_sarima = mean_squared_error(test_data, pred_sarima)
rmse_sarima = np.sqrt(mse_sarima)

# Plot SARIMA results
plt.figure(figsize=(12, 6))
plt.plot(train_data, label='Training Data')
plt.plot(test_data, label='Test Data')
plt.plot(pred_sarima, label='SARIMA Forecast', color='red')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Value")
plt.title("SARIMA Forecast")
plt.show()

print("SARIMA Mean Absolute Error:", mae_sarima)
print("SARIMA Mean Squared Error:", mse_sarima)
print("SARIMA Root Mean Squared Error:", rmse_sarima)

"""## 4. Holt-Winters Exponential Smoothing model"""

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
from statsmodels.tsa.holtwinters import ExponentialSmoothing

# Ensure that you have a datetime index for time series data
# For example, you can load your data like this:
# df = pd.read_csv('your_data.csv', parse_dates=['Date'], index_col='Date')

# Fit the Holt-Winters Exponential Smoothing model
# You can specify the seasonal component (additive or multiplicative) and the frequency (seasonal period)
# For example, for daily data with yearly seasonality:
hwe_model_add_add = ExponentialSmoothing(train_data, seasonal="add", trend="add", seasonal_periods=7).fit()  # Example configuration

# Calculate the length of the test_data
test_data_length = len(test_data)

# Make Holt-Winters predictions
pred_hwe_add_add = hwe_model_add_add.predict(start=test_data.index[0], end=test_data.index[-1])[:test_data_length]

# Calculate evaluation metrics for Holt-Winters
mae_hwe = mean_absolute_error(test_data, pred_hwe_add_add)
mse_hwe = mean_squared_error(test_data, pred_hwe_add_add)
rmse_hwe = np.sqrt(mse_hwe)

# Plot Holt-Winters results
plt.figure(figsize=(12, 6))
plt.plot(train_data, label='Training Data')
plt.plot(test_data, label='Test Data')
plt.plot(pred_hwe_add_add, label='Holt-Winters Forecast', color='red')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Value")
plt.title("Holt-Winters Exponential Smoothing Forecast")
plt.show()

print("Holt-Winters Mean Absolute Error:", mae_hwe)
print("Holt-Winters Mean Squared Error:", mse_hwe)
print("Holt-Winters Root Mean Squared Error:", rmse_hwe)

"""## 5. Random Forest model"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# Ensure that you have a datetime index for time series data

# Assuming you have one column of numeric values
# Create a lag feature (previous value) for time series data
df['Visitors_Lagged'] = df['Visitors'].shift(1)

# Drop missing values resulting from the lag operation
df.dropna(inplace=True)

# Split the data into training and test sets
train_size = int(len(df) * 0.8)
train_data, test_data = df[['Visitors_Lagged']][:train_size], df[['Visitors_Lagged']][train_size:]

# Extract the target variable (the current value)
y_train = df['Visitors'][:train_size]
y_test = df['Visitors'][train_size:]

# Create and train the Random Forest regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=0)  # Adjust the number of trees as needed
rf_model.fit(train_data, y_train)

# Make predictions with the trained Random Forest model
pred_rf = rf_model.predict(test_data)

# Calculate evaluation metrics for Random Forest
mae_rf = mean_absolute_error(y_test, pred_rf)
mse_rf = mean_squared_error(y_test, pred_rf)
rmse_rf = np.sqrt(mse_rf)

# Plot Holt-Winters results
plt.figure(figsize=(12, 6))
plt.plot(train_data, label='Training Data')
plt.plot(test_data, label='Test Data')
plt.plot(y_test.index, pred_rf, label='Random Forest Forecast', color='red')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Visitors")
plt.title("Random Forest Forecast")
plt.show()

print("Random Forest Mean Absolute Error:", mae_rf)
print("Random Forest Mean Squared Error:", mse_rf)
print("Random Forest Root Mean Squared Error:", rmse_rf)

"""## 6. LSTM"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Define hyperparameters
sequence_length = 10  # Length of input sequences
n_epochs = 50  # Number of training epochs
batch_size = 32  # Batch size

# Define a function to create sequences from data
def create_sequences(data, sequence_length):
    sequences = []
    for i in range(len(data) - sequence_length + 1):
        sequence = data[i:i + sequence_length]
        sequences.append(sequence)
    return np.array(sequences)

# Create sequences for LSTM
train_sequences = create_sequences(train_data, sequence_length)
test_sequences = create_sequences(test_data, sequence_length)

# Define and train the LSTM model
model = Sequential([
    LSTM(50, activation='relu', input_shape=(sequence_length, 1)),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')
model.fit(train_sequences, train_sequences[:, -1], epochs=n_epochs, batch_size=batch_size, verbose=2)

# Make LSTM predictions on test data
pred_lstm = []
current_sequence = train_sequences[-1:, :, :]  # Use the last training sequence as the initial input
for _ in range(len(test_sequences)):
    next_value = model.predict(current_sequence)[0, 0]
    pred_lstm.append(next_value)
    current_sequence = np.roll(current_sequence, shift=-1, axis=1)
    current_sequence[0, -1, 0] = next_value

# Truncate test_data to match the length of pred_lstm
test_data_truncated = test_data[sequence_length-1:]

# Calculate evaluation metrics for LSTM
mae_lstm = mean_absolute_error(test_data_truncated, pred_lstm)
mse_lstm = mean_squared_error(test_data_truncated, pred_lstm)
rmse_lstm = np.sqrt(mse_lstm)

# Plot LSTM results
plt.figure(figsize=(12, 6))
plt.plot(train_data, label='Training Data')
plt.plot(test_data, label='Test Data')
plt.plot(range(sequence_length - 1, sequence_length - 1 + len(pred_lstm)), pred_lstm, label='LSTM Forecast', color='red')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Value")
plt.title("LSTM Forecast")
plt.show()

print("LSTM Mean Absolute Error:", mae_lstm)
print("LSTM Mean Squared Error:", mse_lstm)
print("LSTM Root Mean Squared Error:", rmse_lstm)

# Create a Pickle file using serialization
import pickle
pickle_out= open("arima_model1.pkl", "wb")
pickle.dump(arima_model, pickle_out)
pickle_out.close()

'''Create a Pickle file using serialization
import pickle
pickle_out= open("es_model.pkl", "wb")
pickle.dump(es_model, pickle_out)
pickle_out.close()'''

df1.isnull().sum()

df1

daily_data= np.array(df1['Visitors'])
daily_data

np.savetxt("daily_data.csv", daily_data, delimiter=",")







































































































































































































































